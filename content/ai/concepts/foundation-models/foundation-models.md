---
Title: 'Foundation Models'
Description: 'Foundation models power AI systems that are trained on text, images, code, and other formats of data.'
Subjects:
  - 'Machine Learning'
Tags:
  - 'AI'
  - 'NLP'
  - 'Deep Learning'
CatalogContent:
  - 'paths/data-science-nlp'
  - 'intro-to-chatgpt'
---

**Foundation models** power AI systems that are trained on text, images, code, and other formats of data. These models pre-train on large amounts of unlabeled data from the internet.

## Risks

Due to the nature of the data that models train on, biases and stereotypes pose a serious risk. Malicious actors may spread disinformation using AI systems to generate exaggerated content. Misinformation is also a possibility due to artificial hallucinations.

## Existing Models

Some examples of existing models include:

- BERT (Bidirectional Encoder Representations from Transformers) is a model introduced by Google in 2018.
- LaMDA (Language Model for Dialogue Applications) is a model introduced by Google in 2020, and powers Bard which was released in March 2023.
- GPT (Generative Pre-trained Transformer)(https://www.codecademy.com/resources/docs/ai/chatgpt) is a model that was first introduced by OpenAI in 2018. It is trained on text and code, and powers [ChatGPT](https://www.codecademy.com/resources/docs/ai/chatgpt) which was released in November 2022.
- DALL-E is a model introduced in 2021 by OpenAI. It is based on GPT-3 and is used to produce images.

## Model Types

Foundation models are a fundamental part of an AI system. Below are some of the types of models:
