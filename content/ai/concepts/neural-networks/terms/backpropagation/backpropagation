---
Title: 'Backpropagation'
Description: 'Backpropagation is a crucial algorithm in the field of machine learning, specifically in the training of artificial neural networks (ANNs).'
Subjects: 
  - 'Data Science'
  - 'Machine Learning'
Tags: 
  - 'AI'
  - 'Machine Learning'
---

**Backpropagation** is a crucial algorithm in the field of machine learning, specifically in the training of artificial neural networks (ANNs). It enables the neural network to learn and improve its performance by iteratively adjusting the weights and biases of its connections. Here's an example inage, which visualizes the concept of backpropagation:

![backpropagation Explained](https://raw.githubusercontent.com/Codecademy/docs/main/media/Backpropagation_intro.png)

Still Confused? Well this is hard to understand, here's another visualization that will show you multiple layers present in backpropagation based model:

![backpropagation layers](https://raw.githubusercontent.com/Codecademy/docs/main/media/backpropagation_layers.png)

In this document, we will delve into the concept of backpropagation, understand its inner workings, and explore how it is used in an artificial neural network.

## What is Backpropagation?

Backpropagation, short for "backward propagation of errors," is a supervised learning algorithm that calculates the gradient of the loss function with respect to the network's weights. It allows us to determine how much each weight contributes to the overall error or loss of the network's predictions.

## How is Backpropagation Used in Artificial Neural Networks?

Artificial Neural Networks consist of interconnected nodes, called neurons, organized in layers. These layers include an input layer, one or more hidden layers, and an output layer. Backpropagation is used to adjust the weights and biases of the connections between these neurons.

The backpropagation algorithm can be summarized in the following steps:

1. **Forward Pass:** During the forward pass, input data is propagated through the network, layer by layer, until the output layer is reached. Each neuron in a layer receives inputs from the previous layer, calculates a weighted sum, applies an activation function, and passes the result to the next layer.

2. **Calculating the Error:** After the forward pass, the network's output is compared to the expected output using a loss function. The error is the discrepancy between the predicted output and the desired output.

3. **Backward Pass:** In the backward pass, the error is propagated back through the network, starting from the output layer towards the input layer. This is where backpropagation gets its name. The error is assigned to each neuron in proportion to its contribution to the overall error.

4. **Weight and Bias Updates:** Using the calculated errors, the algorithm adjusts the weights and biases of the network's connections. This adjustment is done iteratively, typically using an optimization algorithm like gradient descent, which minimizes the error by updating the weights in the direction opposite to the gradient.

5. **Repeat:** Steps 1 to 4 are repeated for a fixed number of iterations or until a convergence criterion is met. The network gradually learns to minimize the error and improve its predictions.

Here's an illustration that explains the workig of backpropagation:

![Working of backpropagation](https://raw.githubusercontent.com/Codecademy/docs/main/media/Backpropagation%20Working.png)

## Benefits and Importance of Backpropagation

Backpropagation is a fundamental technique in training artificial neural networks due to its numerous benefits:

- **Efficient Training:** Backpropagation allows neural networks to efficiently learn complex relationships in data, making them capable of solving complex problems.

- **Universal Approximators:** ANNs with backpropagation have the ability to approximate any continuous function, given enough neurons and training data.

- **Generalization:** By adjusting weights and biases, backpropagation enables the neural network to generalize from training data to make accurate predictions on unseen data.

- **Adaptability:** Backpropagation allows neural networks to adapt and improve their performance over time, making them suitable for tasks that involve changing environments or evolving data patterns.

- **Deep Learning:** Backpropagation forms the basis of deep learning, enabling the training of deep neural networks with many layers and millions of parameters.

## Example:
Let's consider a simple Backprogression modelwritten in python:

```py
import numpy as np

def sigmoid(x):
    # Sigmoid activation function
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    # Derivative of the sigmoid function
    return sigmoid(x) * (1 - sigmoid(x))

class NeuralNetwork:
    def __init__(self, layers):
        self.layers = layers
        self.weights = []
        self.biases = []
        for i in range(1, len(layers)):
            # Initialize weights and biases for each layer
            w = np.random.randn(layers[i], layers[i-1])
            b = np.zeros((layers[i], 1))
            self.weights.append(w)
            self.biases.append(b)

    def feedforward(self, x):
        # Perform the forward pass through the network
        a = x
        activations = [a]
        for w, b in zip(self.weights, self.biases):
            z = np.dot(w, a) + b
            a = sigmoid(z)
            activations.append(a)
        return activations

    def backpropagation(self, x, y):
        # Perform the backward pass (backpropagation) to update weights and biases
        m = x.shape[1]  # Number of training examples

        # Forward pass
        activations = self.feedforward(x)
        a = activations[-1]  # Output layer activations

        # Backward pass
        delta = a - y  # Calculate initial error

        for i in range(len(self.weights)-1, -1, -1):
            a = activations[i]  # Current layer activations
            z = np.dot(self.weights[i], a) + self.biases[i]
            sigmoid_prime = sigmoid_derivative(z)
            delta = np.dot(self.weights[i].T, delta) * sigmoid_prime

            # Update weights and biases
            self.weights[i] -= (1/m) * np.dot(delta, a.T)
            self.biases[i] -= (1/m) * np.sum(delta, axis=1, keepdims=True)

    def train(self, x, y, epochs, learning_rate):
        for epoch in range(epochs):
            self.backpropagation(x, y)
            if epoch % 100 == 0:
                # Calculate and print the loss every 100 epochs
                loss = np.mean(np.square(self.feedforward(x)[-1] - y))
                print(f"Epoch {epoch}: Loss = {loss}")

# Example usage
# Create a neural network with 2 input neurons, 3 hidden neurons, and 1 output neuron
network = NeuralNetwork([2, 3, 1])

# Define the training data
x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T
y_train = np.array([[0, 1, 1, 0]])

# Train the network
network.train(x_train, y_train, epochs=1000, learning_rate=0.1)

# Test the trained network
x_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T
predictions = network.feedforward(x_test)[-1]
print("Predictions:", predictions)
```

This code demonstrates the implementation of a simple neural network with two input neurons, three hidden neurons, and one output neuron. The network is trained using the XOR logical operation as an example. The backpropagation algorithm is used to update the weights and biases of the network based on the calculated errors. Finally, the trained network is tested on new input data to make predictions.

Please note that this is a basic implementation for demonstration purposes, and in practical scenarios, more advanced techniques, optimizations, and additional layers may be used to achieve better performance.