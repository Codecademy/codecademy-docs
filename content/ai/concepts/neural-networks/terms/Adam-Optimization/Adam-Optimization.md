---
Title: 'adam-optimization' 
Description: 'Adam is a momentum-based optimization algorithm used to iteratively update network weights based in training data.' 
Subjects: 
  - 'AI'
  - 'Machine Learning'
  - 'Computer Science'
Tags: 
  - 'Optimization'
  - 'AI'
  - 'Machine Learning'
  - 'Neural Networks'
  - 'Deep Learning'
  - 'Wegith & Bias'
CatalogContent: 
  - 'machine-learning'
  - 'paths/machine-learning-engineer'
---

[**Adam Optimization** is second-order optimization method that utilize momentum-based algorithms that adapt the learning rate for each parameter based on the first and second moments of the gradent. Adam optimization addresses the limitations of first-order optimization methods such as stochastic gradient decent (SDG), Root Mean Square Propagation (RMSprop), Adagrad, and Adadelta. Ultimately, Adam optimization seeks to achieve faster convergence and better perfromance of deep learning models.]

## Syntax

[Text, code, images, parameters, etc. about the syntax]

## Example

[Text, code, images, etc. about example 1]

## Codebyte Example (if applicable)

We can currently support:

- Python
- JavaScript
- Ruby
- C++
- C#
- Go
- PHP

See [content-standards.md](https://github.com/Codecademy/docs/blob/main/documentation/content-standards.md) for more details!

```codebyte/js
# Example runnable code block.
console.log('Hello, World!');
```
