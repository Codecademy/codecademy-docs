---
Title: 'Linear Models'
Description: 'Linear models are algorithms for regression and classification, modeling the target output as a linear combination of input features.'
Subjects:
  - 'Data Science'
  - 'Machine Learning'
Tags:
  - 'Machine Learning'
  - 'Supervised Learning'
  - 'Unsupervised Learning'
  - 'Scikit-learn'
CatalogContent:
  - 'learn-python-3'
  - 'paths/computer-science'
---

**Linear models** are foundational algorithms in machine learning for both regression and classification tasks. These models assume that the target variable can be expressed as a linear combination of input features, making them simple yet effective for many datasets. In regression, the output is continuous, while in classification, a decision boundary is determined based on a linear relationship. Scikit-learn provides a wide range of linear models, including Linear Regression, Logistic Regression, Ridge, Lasso, and Elastic Net, among others.

Scikit-learn offers a variety of linear models, each suited to different scenarios:

- **Linear Regression**: Ideal for predicting continuous outputs by minimizing the sum of squared errors.
- **Ridge and Lasso Regression**: Regularized linear models that address overfitting; Ridge uses L2 regularization, while Lasso employs L1 regularization for feature selection.
- **Elastic Net**: Combines L1 and L2 regularization for a balance between Ridge and Lasso.
- **Logistic Regression**: A classification algorithm using a logistic (sigmoid) function to handle binary or multiclass problems.
- **Bayesian Ridge Regression**: Incorporates probabilistic modeling for regression, providing uncertainty estimates.
- **SGD Regressor and Classifier**: Use stochastic gradient descent for scalable learning on large datasets.
- **Perceptron**: A basic linear classifier based on a single-layer perceptron algorithm.
- **Passive-Aggressive Regressor and Classifier**: Efficient for online learning and datasets arriving incrementally.

These algorithms make linear models highly versatile and widely applicable across domains, from predicting house prices to determining customer churn.

## Syntax

Linear models in scikit-learn are implemented through various algorithms, each suited for specific use cases. Below is the syntax for one such algorithm, Linear Regression, a regression model that minimizes the sum of squared errors:

```pseudo
from sklearn.linear_model import LinearRegression

# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model on training data
model.fit(X_train, y_train)

# Predict on new data
y_pred = model.predict(X_test)
```

## Example

Hereâ€™s a practical example using Linear Regression, one of the core algorithms in scikit-learn's linear models:

```py
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression

# Generate a synthetic regression dataset
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# Split data into training and testing 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict and display results
y_pred = model.predict(X_test)
print("Predicted values:", y_pred)
```

The code above produces the following possible output:

```shell
Predicted values: [-58.66528327  65.48743554  36.04876271 -17.24928234 -10.26070235
 -12.80652919 -20.50660984 -77.90504757  36.46362633  41.30294944
  45.71495289  15.89937588 -53.91600662 -23.33958474 -50.82554729
 -21.10145005  43.26141852  10.80464503  32.7720721   16.72167377]
```

The output of the code will be the predicted values for the test dataset generated by the Linear Regression model. Since the data is synthetic, the exact values depend on the randomly generated dataset.

In this example, Linear Regression is used to model a simple synthetic dataset. The same principles apply to other algorithms within the linear models family, such as Ridge, Lasso, and Logistic Regression, depending on the task.
